{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_json\" function to read\n",
    "# the labeled training data\n",
    "# WARNING: Training the model takes about 10-25 mins, because vocab size was set to 10 000 and number of trees to 100\n",
    "import pandas as pd  \n",
    "import re     \n",
    "import nltk\n",
    "import json\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_json('train.json', encoding=('utf-8'))\n",
    "\n",
    "\n",
    "def text_to_words( raw_text ):\n",
    "    # Function to convert a raw text to a string of words\n",
    "    # The input is a single string (a raw text), and \n",
    "    # the output is a single string (a preprocessed text)\n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^а-яА-Я]\", \" \", raw_text) \n",
    "   \n",
    "\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "\n",
    "                                 \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"russian\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "# Get the number of texts based on the dataframe column size\n",
    "num_texts = train[\"text\"].size\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the clean texts\n",
    "print(\"Cleaning and parsing the training set texts...\\n\")\n",
    "clean_train_texts = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the text list \n",
    "for i in range( 0, num_texts ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "    \tprint(\"Text %d of %d\\n\" % ( i+1, num_texts ))                                                                    \n",
    "    clean_train_texts.append( text_to_words( train[\"text\"][i] ))\n",
    "\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_texts)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"text\":clean_train_texts[:2000], \"sentiment\":train[\"sentiment\"][:2000]} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"train2.csv\", index=False, quoting=3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"text\":clean_train_texts[:500], \"sentiment\":train[\"sentiment\"][:500]} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"train1.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
