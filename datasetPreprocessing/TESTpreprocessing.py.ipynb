{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set texts...\n",
      "\n",
      "Text 1000 of 8263\n",
      "\n",
      "Text 2000 of 8263\n",
      "\n",
      "Text 3000 of 8263\n",
      "\n",
      "Text 4000 of 8263\n",
      "\n",
      "Text 5000 of 8263\n",
      "\n",
      "Text 6000 of 8263\n",
      "\n",
      "Text 7000 of 8263\n",
      "\n",
      "Text 8000 of 8263\n",
      "\n",
      "Creating the bag of words...\n",
      "\n",
      "(2056, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 2056\n",
      "\n",
      "Review 2000 of 2056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the pandas package, then use the \"read_json\" function to read\n",
    "# the labeled training data\n",
    "# WARNING: Training the model takes about 10-25 mins, because vocab size was set to 10 000 and number of trees to 100\n",
    "import pandas as pd  \n",
    "import re     \n",
    "import nltk\n",
    "import json\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_json('train.json', encoding=('utf-8'))\n",
    "\n",
    "\n",
    "def text_to_words( raw_text ):\n",
    "    # Function to convert a raw text to a string of words\n",
    "    # The input is a single string (a raw text), and \n",
    "    # the output is a single string (a preprocessed text)\n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^а-яА-Я]\", \" \", raw_text) \n",
    "   \n",
    "\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "\n",
    "                                 \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"russian\"))                  \n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "# Get the number of texts based on the dataframe column size\n",
    "num_texts = train[\"text\"].size\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the clean texts\n",
    "print(\"Cleaning and parsing the training set texts...\\n\")\n",
    "clean_train_texts = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the text list \n",
    "for i in range( 0, num_texts ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "    \tprint(\"Text %d of %d\\n\" % ( i+1, num_texts ))                                                                    \n",
    "    clean_train_texts.append( text_to_words( train[\"text\"][i] ))\n",
    "\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_texts)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "test = pd.read_json('test.json', encoding=('utf-8'))\n",
    "\n",
    "#test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "#                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_texts = len(test[\"text\"])\n",
    "clean_test_texts = [] \n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_texts):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_texts))\n",
    "    clean_text = text_to_words( test[\"text\"][i] )\n",
    "    clean_test_texts.append( clean_text )\n",
    "\n",
    "d = pd.read_csv('data.csv', encoding=('utf-8'))\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"text\":clean_test_texts, \"sentiment\":d[\"sentiment\"]} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
